<!DOCTYPE html><html lang="en" data-theme="dark"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>数模算法总结 | TnTeQAQ的blog</title><meta name="author" content="TnTeQAQ"><meta name="copyright" content="TnTeQAQ"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#0d0d0d"><meta name="description" content="数模算法总结">
<meta property="og:type" content="article">
<meta property="og:title" content="数模算法总结">
<meta property="og:url" content="https://tnteqaq.github.io/2024/01/30/%E6%95%B0%E6%A8%A1%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/index.html">
<meta property="og:site_name" content="TnTeQAQ的blog">
<meta property="og:description" content="数模算法总结">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://tnteqaq.github.io/img/megumin.jpg">
<meta property="article:published_time" content="2024-01-30T15:58:29.000Z">
<meta property="article:modified_time" content="2024-02-01T06:42:28.172Z">
<meta property="article:author" content="TnTeQAQ">
<meta property="article:tag" content="数据挖掘">
<meta property="article:tag" content="算法">
<meta property="article:tag" content="数据分析">
<meta property="article:tag" content="数模">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tnteqaq.github.io/img/megumin.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://tnteqaq.github.io/2024/01/30/%E6%95%B0%E6%A8%A1%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '数模算法总结',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-02-01 14:42:28'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/megumin.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">25</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">40</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">2</div></a></div><hr class="custom-hr"/></div></div><div class="post" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="TnTeQAQ的blog"><span class="site-name">TnTeQAQ的blog</span></a></span><div id="menus"><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">数模算法总结</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-01-30T15:58:29.000Z" title="Created 2024-01-30 23:58:29">2024-01-30</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-02-01T06:42:28.172Z" title="Updated 2024-02-01 14:42:28">2024-02-01</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%87%E5%BF%98%E5%BD%95/">备忘录</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="数模算法总结"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><p>数模算法总结</p>
<span id="more"></span>
<blockquote>
<p>下面这些算法分类并不绝对，有的算法可能同时属于好几类，但是只写在一类里。</p>
</blockquote>
<h1 id="算法类型"><a href="#算法类型" class="headerlink" title="算法类型"></a>算法类型</h1><h2 id="矩阵分解"><a href="#矩阵分解" class="headerlink" title="矩阵分解"></a>矩阵分解</h2><p><img src="/2024/01/30/%E6%95%B0%E6%A8%A1%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/image-20240131031645231.png" alt="image-20240131031645231"></p>
<p><img src="/2024/01/30/%E6%95%B0%E6%A8%A1%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/%E5%8E%86%E5%8F%B2.jpg" alt="历史"></p>
<p>（摘自<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/Dv51K8JETakIKe5dPBAPVg">机器学习中SVD总结 (qq.com)</a>）</p>
<h3 id="奇异值分解"><a href="#奇异值分解" class="headerlink" title="奇异值分解"></a>奇异值分解</h3><p>SVD，被用于PCA算法。</p>
<h3 id="主成分分析"><a href="#主成分分析" class="headerlink" title="主成分分析"></a>主成分分析</h3><p>PCA数据降维。</p>
<h2 id="属性约简算法"><a href="#属性约简算法" class="headerlink" title="属性约简算法"></a>属性约简算法</h2><h3 id="Relief算法"><a href="#Relief算法" class="headerlink" title="Relief算法"></a>Relief算法</h3><p>基于邻近实例的思想。</p>
<h3 id="RoughSets算法"><a href="#RoughSets算法" class="headerlink" title="RoughSets算法"></a>RoughSets算法</h3><p>粗糙集理论。</p>
<h2 id="关联规则挖掘算法"><a href="#关联规则挖掘算法" class="headerlink" title="关联规则挖掘算法"></a>关联规则挖掘算法</h2><p>可以用来找出数据集中频繁出现的数据集合。</p>
<p>比如在一堆的购物清单中，找出 {牛奶、面包} 经常被一起购买。</p>
<p>拓展一下，用于判断单词中哪个词根词缀出现的最多。</p>
<h3 id="Apriori算法"><a href="#Apriori算法" class="headerlink" title="Apriori算法"></a>Apriori算法</h3><p>通过对数据<em>逐层迭代</em>的方法，比较耗时。</p>
<p>产生大量的候选项目集，会消耗大量的内存。</p>
<h3 id="AprioriAll算法"><a href="#AprioriAll算法" class="headerlink" title="AprioriAll算法"></a>AprioriAll算法</h3><p>改进Apriori算法，通过候选序列的逐级扩展和剪枝策略来减少搜索空间。</p>
<h3 id="GSP算法"><a href="#GSP算法" class="headerlink" title="GSP算法"></a>GSP算法</h3><p>与AprioriAll算法相比，GSP算法计算较少的候选集，并且在数据转换中不需要事先计算频繁集。</p>
<p>对于序列模式的长度比较长的情况，算法难以处理。</p>
<h3 id="PCY算法"><a href="#PCY算法" class="headerlink" title="PCY算法"></a>PCY算法</h3><p><strong>改进Apriori算法</strong>，引进了<em>哈希函数</em>，降低Apriori算法频繁的扫描数据库，降低候选项集所占用的内存。</p>
<p><em>由于哈希函数的原因，有些非频繁项被映射到了频繁桶中，导致这部分非频繁项很难被过滤掉</em></p>
<h3 id="多阶段算法"><a href="#多阶段算法" class="headerlink" title="多阶段算法"></a>多阶段算法</h3><p><strong>改进PCY算法</strong>，解决PCY算法非频繁项被映射到了频繁桶中的问题。</p>
<h3 id="多哈希算法"><a href="#多哈希算法" class="headerlink" title="多哈希算法"></a>多哈希算法</h3><p><strong>与PCY算法基本一致</strong>，使用多个哈希表。</p>
<h3 id="FP-Tree算法"><a href="#FP-Tree算法" class="headerlink" title="FP-Tree算法"></a>FP-Tree算法</h3><p>对数据库进行两次扫描便可以对所有可能的频繁项进行计数（更快）。</p>
<p>FP-Tree算法主要分为两个步骤：构建FP树和频繁模式挖掘。</p>
<h3 id="XFP-Tree算法"><a href="#XFP-Tree算法" class="headerlink" title="XFP-Tree算法"></a>XFP-Tree算法</h3><p><strong>改进FP-Tree算法</strong>，XFP-Tree算法旨在<em>并行</em>的构建FP树来提升效率（更更快）。</p>
<h3 id="PrefixSpan算法"><a href="#PrefixSpan算法" class="headerlink" title="PrefixSpan算法"></a>PrefixSpan算法</h3><p>高效但内存消耗较大。</p>
<h2 id="分类与回归算法"><a href="#分类与回归算法" class="headerlink" title="分类与回归算法"></a>分类与回归算法</h2><h3 id="ID3算法"><a href="#ID3算法" class="headerlink" title="ID3算法"></a>ID3算法</h3><p>信息增益选择特征。</p>
<p>ID3算法的核心是根据<em>信息增益</em>来选择进行划分的特征，然后递归地构建<strong>决策树</strong>。</p>
<h3 id="C4-5算法"><a href="#C4-5算法" class="headerlink" title="C4.5算法"></a>C4.5算法</h3><p>信息增益率选择特征。</p>
<p><strong>ID3算法的一种延伸和优化</strong>。</p>
<h3 id="CART算法"><a href="#CART算法" class="headerlink" title="CART算法"></a>CART算法</h3><p>基尼系数选择特征。</p>
<p>能够处理连续型特征和离散型特征。</p>
<p>容易过拟合，需要设置合适的阈值控制树的深度；<br>对于缺失值的数据，需要进行填充或忽略处理。</p>
<h4 id="分类决策树"><a href="#分类决策树" class="headerlink" title="分类决策树"></a>分类决策树</h4><p>对于大规模数据集，构建决策树的时间复杂度较高。</p>
<h4 id="回归树"><a href="#回归树" class="headerlink" title="回归树"></a>回归树</h4><p>对于非线性数据或噪声数据，回归树的预测精度可能较低。</p>
<h3 id="K-NN算法"><a href="#K-NN算法" class="headerlink" title="K-NN算法"></a>K-NN算法</h3><p>可以用来做<strong>分类</strong>也可以用来做<strong>回归</strong>。</p>
<p>计算量大。</p>
<h3 id="贝叶斯算法"><a href="#贝叶斯算法" class="headerlink" title="贝叶斯算法"></a>贝叶斯算法</h3><h4 id="朴素贝叶斯分类算法"><a href="#朴素贝叶斯分类算法" class="headerlink" title="朴素贝叶斯分类算法"></a>朴素贝叶斯分类算法</h4><p>过滤垃圾邮件。</p>
<p>朴素贝叶斯法是基于贝叶斯定理与特征条件独立性假设的分类方法。</p>
<h4 id="贝叶斯回归"><a href="#贝叶斯回归" class="headerlink" title="贝叶斯回归"></a>贝叶斯回归</h4><p>叶斯回归不仅给出了预测值，同时也提供了预测的不确定性（即<strong>预测值的分布</strong>）。</p>
<h3 id="关联规则分类算法（CBA）"><a href="#关联规则分类算法（CBA）" class="headerlink" title="关联规则分类算法（CBA）"></a>关联规则分类算法（CBA）</h3><p>利用了Apriori挖掘出的关联规则，然后做分类判断。</p>
<h3 id="支持向量机（SVM）"><a href="#支持向量机（SVM）" class="headerlink" title="支持向量机（SVM）"></a>支持向量机（SVM）</h3><p>支持线性、非线性。</p>
<h3 id="随机森林算法"><a href="#随机森林算法" class="headerlink" title="随机森林算法"></a>随机森林算法</h3><p>噪声较大的样本集容易过拟合。</p>
<h2 id="聚类算法"><a href="#聚类算法" class="headerlink" title="聚类算法"></a>聚类算法</h2><p>无标签。</p>
<p><img src="/2024/01/30/%E6%95%B0%E6%A8%A1%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95%E6%AF%94%E8%BE%83.png" alt="聚类方法比较"></p>
<h3 id="k-Means算法"><a href="#k-Means算法" class="headerlink" title="k-Means算法"></a>k-Means算法</h3><p>对异常数据敏感</p>
<h3 id="k-means-算法"><a href="#k-means-算法" class="headerlink" title="k-means++算法"></a>k-means++算法</h3><p>对k-means中初始质心点选取的优化算法。</p>
<h3 id="bi-kmeans算法"><a href="#bi-kmeans算法" class="headerlink" title="bi-kmeans算法"></a>bi-kmeans算法</h3><p>对kmeans算法会陷入局部最优的缺陷进行的改进算法。</p>
<h3 id="模糊C均值聚类算法"><a href="#模糊C均值聚类算法" class="headerlink" title="模糊C均值聚类算法"></a>模糊C均值聚类算法</h3><p>软聚类。</p>
<h3 id="期望最大化算法（EM）"><a href="#期望最大化算法（EM）" class="headerlink" title="期望最大化算法（EM）"></a>期望最大化算法（EM）</h3><p>一种迭代优化算法，用于估计概率模型的参数。用于隐马尔可夫模型（HMM）、高斯混合模型（GMM）等。</p>
<h3 id="隐马尔可夫模型（HMM）"><a href="#隐马尔可夫模型（HMM）" class="headerlink" title="隐马尔可夫模型（HMM）"></a>隐马尔可夫模型（HMM）</h3><p>可以用于无监督学习和监督学习。</p>
<h3 id="高斯混合模型（GMM）"><a href="#高斯混合模型（GMM）" class="headerlink" title="高斯混合模型（GMM）"></a>高斯混合模型（GMM）</h3><p>适用于软聚类和复杂的簇形状。</p>
<h3 id="DBSCAN密度聚类"><a href="#DBSCAN密度聚类" class="headerlink" title="DBSCAN密度聚类"></a>DBSCAN密度聚类</h3><p>对初值选取敏感，对噪声不敏感。</p>
<p>对密度不均的数据聚合效果不好。</p>
<h3 id="OPTICS密度聚类"><a href="#OPTICS密度聚类" class="headerlink" title="OPTICS密度聚类"></a>OPTICS密度聚类</h3><p>对DBSCAN算法的一种有效扩展，主要解决对输入参数敏感的问题。</p>
<p>DBSCAN效果</p>
<p><img src="/2024/01/30/%E6%95%B0%E6%A8%A1%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/DBSCAN.webp" alt="DBSCAN"></p>
<p>OPTICS效果</p>
<p><img src="/2024/01/30/%E6%95%B0%E6%A8%A1%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/OPTICS.jpg" alt="OPTICS"></p>
<h3 id="Agglomerative层次聚类"><a href="#Agglomerative层次聚类" class="headerlink" title="Agglomerative层次聚类"></a>Agglomerative层次聚类</h3><p>又称自底向上的层次聚类。</p>
<h3 id="Divisive层次聚类"><a href="#Divisive层次聚类" class="headerlink" title="Divisive层次聚类"></a>Divisive层次聚类</h3><p>又称自顶向下的层次聚类。</p>
<h2 id="图算法"><a href="#图算法" class="headerlink" title="图算法"></a>图算法</h2><h3 id="频繁子图挖掘算法"><a href="#频繁子图挖掘算法" class="headerlink" title="频繁子图挖掘算法"></a>频繁子图挖掘算法</h3><p>gSpan算法、DFS 编码。</p>
<h3 id="HITS算法"><a href="#HITS算法" class="headerlink" title="HITS算法"></a>HITS算法</h3><p>Hub页面（枢纽页面）、Authority页面（权威页面）。</p>
<p>计算效率较低，主题漂移问题，结构不稳定。</p>
<h3 id="PageRank算法"><a href="#PageRank算法" class="headerlink" title="PageRank算法"></a>PageRank算法</h3><p>马尔可夫链、计算权重。</p>
<h2 id="寻找最优解"><a href="#寻找最优解" class="headerlink" title="寻找最优解"></a>寻找最优解</h2><h3 id="遗传算法（GA）"><a href="#遗传算法（GA）" class="headerlink" title="遗传算法（GA）"></a>遗传算法（GA）</h3><p>基因编码。</p>
<h3 id="蚁群算法（ACO）"><a href="#蚁群算法（ACO）" class="headerlink" title="蚁群算法（ACO）"></a>蚁群算法（ACO）</h3><p>不容易陷入局部最优。</p>
<p>收敛速度慢。</p>
<h3 id="维特比算法（viterbi）"><a href="#维特比算法（viterbi）" class="headerlink" title="维特比算法（viterbi）"></a>维特比算法（viterbi）</h3><p>动态规划。</p>
<h3 id="模拟退火算法"><a href="#模拟退火算法" class="headerlink" title="模拟退火算法"></a>模拟退火算法</h3><p>搜索时间越长，获得的最优解更可靠。</p>
<h1 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h1><h2 id="评价回归"><a href="#评价回归" class="headerlink" title="评价回归"></a>评价回归</h2><h3 id="SSE（残差平方和、和方差）"><a href="#SSE（残差平方和、和方差）" class="headerlink" title="SSE（残差平方和、和方差）"></a>SSE（残差平方和、和方差）</h3><p>数据和原始数据对应点的误差的平方和</p>
<h3 id="MSE（均方差、方差）"><a href="#MSE（均方差、方差）" class="headerlink" title="MSE（均方差、方差）"></a>MSE（均方差、方差）</h3><p>预测值与真实值之间差异的平方和的平均值，越小越好。</p>
<h3 id="RMSE（均方根、标准差）"><a href="#RMSE（均方根、标准差）" class="headerlink" title="RMSE（均方根、标准差）"></a>RMSE（均方根、标准差）</h3><p>MSE的平方根，越小越好。</p>
<h3 id="MAE（平均绝对误差）"><a href="#MAE（平均绝对误差）" class="headerlink" title="MAE（平均绝对误差）"></a>MAE（平均绝对误差）</h3><p>预测值与真实值之间差异的绝对值的平均值，越小越好。</p>
<h3 id="MPAE（平均绝对百分比误差）"><a href="#MPAE（平均绝对百分比误差）" class="headerlink" title="MPAE（平均绝对百分比误差）"></a>MPAE（平均绝对百分比误差）</h3><p>可以比较两个不同尺度的数据之间的差异，越小越好。</p>
<script type="math/tex; mode=display">
M = \frac{1}{n}\sum_{t=1}^n\lvert \frac{A_t-F_t}{A_t} \rvert</script><p><img src="/2024/01/30/%E6%95%B0%E6%A8%A1%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/insights_mape_example.jpg" alt="insights_mape_example"></p>
<h3 id="sMAPE-对称平均绝对百分比误差"><a href="#sMAPE-对称平均绝对百分比误差" class="headerlink" title="sMAPE 对称平均绝对百分比误差"></a>sMAPE 对称平均绝对百分比误差</h3><p>改进MAPE。</p>
<script type="math/tex; mode=display">
SMAPE = \frac{1}{n}\sum_{t=1}^n\lvert \frac{F_t-A_t}{(A_t+F_t)/2} \rvert</script><h3 id="R-squared（确定系数）"><a href="#R-squared（确定系数）" class="headerlink" title="R-squared（确定系数）"></a>R-squared（确定系数）</h3><p>通过 SSR（回归平方和）、SST（总偏差平方和）计算得，取值范围为[0 1]，越接近1越好。</p>
<h3 id="Adjusted-R-squared（调整R方）"><a href="#Adjusted-R-squared（调整R方）" class="headerlink" title="Adjusted R-squared（调整R方）"></a>Adjusted R-squared（调整R方）</h3><p>单变量线性回归，则使用R-squared评估，多变量，则使用adjusted R-squared。</p>
<h3 id="Huber-Loss（霍尔伯损失）"><a href="#Huber-Loss（霍尔伯损失）" class="headerlink" title="Huber Loss（霍尔伯损失）"></a>Huber Loss（霍尔伯损失）</h3><p>对MSE和MAE的一种折中，它在预测值与真实值相差较大时使用MAE，而在预测值与真实值相差较小时使用MSE。</p>
<h2 id="评价聚类"><a href="#评价聚类" class="headerlink" title="评价聚类"></a>评价聚类</h2><p><img src="/2024/01/30/%E6%95%B0%E6%A8%A1%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/image-20240201134016702.png" alt="image-20240201134016702"></p>
<h3 id="轮廓系数"><a href="#轮廓系数" class="headerlink" title="轮廓系数"></a>轮廓系数</h3><p>轮廓系数S的取值范围为[-1, 1]，越大越好。</p>
<h3 id="Calinski-Harabasz指数（方差比准则）"><a href="#Calinski-Harabasz指数（方差比准则）" class="headerlink" title="Calinski-Harabasz指数（方差比准则）"></a>Calinski-Harabasz指数（方差比准则）</h3><p>取值范围在(0,+∞)之间，越大越好。</p>
<h3 id="Davies-Bouldin-Index（DB指数）"><a href="#Davies-Bouldin-Index（DB指数）" class="headerlink" title="Davies-Bouldin Index（DB指数）"></a>Davies-Bouldin Index（DB指数）</h3><p>取值范围在[0,+∞)之间，越小越好。</p>
<h3 id="Rand-Index（兰德指数）"><a href="#Rand-Index（兰德指数）" class="headerlink" title="Rand Index（兰德指数）"></a>Rand Index（兰德指数）</h3><p>范围从0到1,1的值表示两个聚类完全相同，接近0的值表示两个聚类有很大的不同。</p>
<h3 id="Adjusted-Rand-Score（调整兰德指数）"><a href="#Adjusted-Rand-Score（调整兰德指数）" class="headerlink" title="Adjusted Rand Score（调整兰德指数）"></a>Adjusted Rand Score（调整兰德指数）</h3><p>取值范围为[-1, 1]，其中1表示聚类结果与真实标签完全一致，0表示聚类结果与真实标签随机一致，-1表示聚类结果与真实标签完全不一致。</p>
<h3 id="Normalized-Mutual-Information-Score（标准化互信息分数）"><a href="#Normalized-Mutual-Information-Score（标准化互信息分数）" class="headerlink" title="Normalized Mutual Information Score（标准化互信息分数）"></a>Normalized Mutual Information Score（标准化互信息分数）</h3><p>对互信息的标准化，取值范围为0（没有互信息）到1（完全相关）。</p>
<h3 id="Adjusted-Mutual-Information-Score（调整互信息分数）"><a href="#Adjusted-Mutual-Information-Score（调整互信息分数）" class="headerlink" title="Adjusted Mutual Information Score（调整互信息分数）"></a>Adjusted Mutual Information Score（调整互信息分数）</h3><p>调整互信息分数更加稳健，不受标签排序的影响。</p>
<h3 id="Homogeneity-and-Completeness-Score（同质性和完整性）"><a href="#Homogeneity-and-Completeness-Score（同质性和完整性）" class="headerlink" title="Homogeneity and Completeness Score（同质性和完整性）"></a>Homogeneity and Completeness Score（同质性和完整性）</h3><p>越大越好。</p>
<h3 id="V-Measure"><a href="#V-Measure" class="headerlink" title="V-Measure"></a>V-Measure</h3><p>取值范围为[0, 1]，越大越好。</p>
<p>V-Measure对于聚类结果的不确定性更加敏感。</p>
<h3 id="Fowlkes-Mallows-Score"><a href="#Fowlkes-Mallows-Score" class="headerlink" title="Fowlkes-Mallows Score"></a>Fowlkes-Mallows Score</h3><p>基于数据的真实标签和聚类结果的交集、联合集以及簇内和簇间点对数的比值来计算。</p>
<h3 id="邓恩（dunn）指标"><a href="#邓恩（dunn）指标" class="headerlink" title="邓恩（dunn）指标"></a>邓恩（dunn）指标</h3><p>取值范围为[0, ∞)，越大代表类间距越大、同时类内间距越小。</p>
<h2 id="评价分类"><a href="#评价分类" class="headerlink" title="评价分类"></a>评价分类</h2><h3 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h3><p>各种评价指标的基础。</p>
<p><img src="/2024/01/30/%E6%95%B0%E6%A8%A1%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5.png" alt="混淆矩阵"></p>
<ul>
<li>真正例（True Positive，TP）：一个正例被正确预测为正例。</li>
<li>真反例（True Negative，TN）：一个反例被正确预测为反例。</li>
<li>假正例（False Positive，FP）：一个反例被错误预测为正例。</li>
<li>假反例（False Negative，FN）：一个正例被错误预测为反例。</li>
</ul>
<h3 id="准确率（Accuracy）"><a href="#准确率（Accuracy）" class="headerlink" title="准确率（Accuracy）"></a>准确率（Accuracy）</h3><p>有样本中预测正确的样本占比。</p>
<h3 id="精确率（Precision）"><a href="#精确率（Precision）" class="headerlink" title="精确率（Precision）"></a>精确率（Precision）</h3><p>在所有被预测为正的样本中实际为正的样本的概率。</p>
<h3 id="召回率（Recall）"><a href="#召回率（Recall）" class="headerlink" title="召回率（Recall）"></a>召回率（Recall）</h3><p>在实际为正的样本中被预测为正样本的概率。</p>
<h3 id="F1-Score"><a href="#F1-Score" class="headerlink" title="F1-Score"></a>F1-Score</h3><p>F-Measure是P和R的加权调和平均。</p>
<h3 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a>ROC曲线</h3><p>将真阳性率（True Positive Rate，TPR）和假阳性率（False Positive Rate，FPR）作为横纵坐标来描绘分类器在不同阈值下的性能。</p>
<h3 id="AUC值"><a href="#AUC值" class="headerlink" title="AUC值"></a>AUC值</h3><p>AUC是ROC曲线下的面积。</p>
<p>完美的分类器的AUC值为1，而随机分类器的AUC值为0.5。</p>
<h3 id="Hinge-Loss（铰链损失）"><a href="#Hinge-Loss（铰链损失）" class="headerlink" title="Hinge Loss（铰链损失）"></a>Hinge Loss（铰链损失）</h3><p>一种常用于支持向量机（SVM）的损失函数。</p>
<h3 id="Log-Loss（对数损失）"><a href="#Log-Loss（对数损失）" class="headerlink" title="Log Loss（对数损失）"></a>Log Loss（对数损失）</h3><p>常用的分类问题的损失函数。计算的是模型预测概率与真实标签的对数似然。</p>
<h3 id="Softmax交叉熵损失"><a href="#Softmax交叉熵损失" class="headerlink" title="Softmax交叉熵损失"></a>Softmax交叉熵损失</h3><p>使用softmax函数将模型预测值转换为概率分布，然后计算该概率分布与真实标签分布之间的差异。</p>
<h3 id="kappa系数"><a href="#kappa系数" class="headerlink" title="kappa系数"></a>kappa系数</h3><p>取值为-1到1之间,通常大于0。</p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/">数据挖掘</a><a class="post-meta__tags" href="/tags/%E7%AE%97%E6%B3%95/">算法</a><a class="post-meta__tags" href="/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/">数据分析</a><a class="post-meta__tags" href="/tags/%E6%95%B0%E6%A8%A1/">数模</a></div><div class="post_share"><div class="social-share" data-image="/img/megumin.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/02/17/%E7%94%A8%E6%88%B7%E4%B8%AD%E5%BF%83%E9%A1%B9%E7%9B%AE%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/" title="用户中心项目实践笔记"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">用户中心项目实践笔记</div></div></a></div><div class="next-post pull-right"><a href="/2024/01/29/%E6%9C%80%E8%BF%91%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%BB%E7%BB%93/" title="最近的一些总结"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Next</div><div class="next_info">最近的一些总结</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/megumin.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">TnTeQAQ</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">25</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">40</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/TnTeQAQ"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E7%B1%BB%E5%9E%8B"><span class="toc-number">1.</span> <span class="toc-text">算法类型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3"><span class="toc-number">1.1.</span> <span class="toc-text">矩阵分解</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3"><span class="toc-number">1.1.1.</span> <span class="toc-text">奇异值分解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90"><span class="toc-number">1.1.2.</span> <span class="toc-text">主成分分析</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B1%9E%E6%80%A7%E7%BA%A6%E7%AE%80%E7%AE%97%E6%B3%95"><span class="toc-number">1.2.</span> <span class="toc-text">属性约简算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Relief%E7%AE%97%E6%B3%95"><span class="toc-number">1.2.1.</span> <span class="toc-text">Relief算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RoughSets%E7%AE%97%E6%B3%95"><span class="toc-number">1.2.2.</span> <span class="toc-text">RoughSets算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%8C%96%E6%8E%98%E7%AE%97%E6%B3%95"><span class="toc-number">1.3.</span> <span class="toc-text">关联规则挖掘算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Apriori%E7%AE%97%E6%B3%95"><span class="toc-number">1.3.1.</span> <span class="toc-text">Apriori算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AprioriAll%E7%AE%97%E6%B3%95"><span class="toc-number">1.3.2.</span> <span class="toc-text">AprioriAll算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GSP%E7%AE%97%E6%B3%95"><span class="toc-number">1.3.3.</span> <span class="toc-text">GSP算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PCY%E7%AE%97%E6%B3%95"><span class="toc-number">1.3.4.</span> <span class="toc-text">PCY算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E9%98%B6%E6%AE%B5%E7%AE%97%E6%B3%95"><span class="toc-number">1.3.5.</span> <span class="toc-text">多阶段算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E5%93%88%E5%B8%8C%E7%AE%97%E6%B3%95"><span class="toc-number">1.3.6.</span> <span class="toc-text">多哈希算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#FP-Tree%E7%AE%97%E6%B3%95"><span class="toc-number">1.3.7.</span> <span class="toc-text">FP-Tree算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#XFP-Tree%E7%AE%97%E6%B3%95"><span class="toc-number">1.3.8.</span> <span class="toc-text">XFP-Tree算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PrefixSpan%E7%AE%97%E6%B3%95"><span class="toc-number">1.3.9.</span> <span class="toc-text">PrefixSpan算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E4%B8%8E%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95"><span class="toc-number">1.4.</span> <span class="toc-text">分类与回归算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ID3%E7%AE%97%E6%B3%95"><span class="toc-number">1.4.1.</span> <span class="toc-text">ID3算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#C4-5%E7%AE%97%E6%B3%95"><span class="toc-number">1.4.2.</span> <span class="toc-text">C4.5算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CART%E7%AE%97%E6%B3%95"><span class="toc-number">1.4.3.</span> <span class="toc-text">CART算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">1.4.3.1.</span> <span class="toc-text">分类决策树</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E6%A0%91"><span class="toc-number">1.4.3.2.</span> <span class="toc-text">回归树</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#K-NN%E7%AE%97%E6%B3%95"><span class="toc-number">1.4.4.</span> <span class="toc-text">K-NN算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95"><span class="toc-number">1.4.5.</span> <span class="toc-text">贝叶斯算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95"><span class="toc-number">1.4.5.1.</span> <span class="toc-text">朴素贝叶斯分类算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%9B%9E%E5%BD%92"><span class="toc-number">1.4.5.2.</span> <span class="toc-text">贝叶斯回归</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%EF%BC%88CBA%EF%BC%89"><span class="toc-number">1.4.6.</span> <span class="toc-text">关联规则分类算法（CBA）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89"><span class="toc-number">1.4.7.</span> <span class="toc-text">支持向量机（SVM）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%AE%97%E6%B3%95"><span class="toc-number">1.4.8.</span> <span class="toc-text">随机森林算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95"><span class="toc-number">1.5.</span> <span class="toc-text">聚类算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#k-Means%E7%AE%97%E6%B3%95"><span class="toc-number">1.5.1.</span> <span class="toc-text">k-Means算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#k-means-%E7%AE%97%E6%B3%95"><span class="toc-number">1.5.2.</span> <span class="toc-text">k-means++算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#bi-kmeans%E7%AE%97%E6%B3%95"><span class="toc-number">1.5.3.</span> <span class="toc-text">bi-kmeans算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E7%B3%8AC%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95"><span class="toc-number">1.5.4.</span> <span class="toc-text">模糊C均值聚类算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7%E5%8C%96%E7%AE%97%E6%B3%95%EF%BC%88EM%EF%BC%89"><span class="toc-number">1.5.5.</span> <span class="toc-text">期望最大化算法（EM）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%EF%BC%88HMM%EF%BC%89"><span class="toc-number">1.5.6.</span> <span class="toc-text">隐马尔可夫模型（HMM）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%EF%BC%88GMM%EF%BC%89"><span class="toc-number">1.5.7.</span> <span class="toc-text">高斯混合模型（GMM）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DBSCAN%E5%AF%86%E5%BA%A6%E8%81%9A%E7%B1%BB"><span class="toc-number">1.5.8.</span> <span class="toc-text">DBSCAN密度聚类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#OPTICS%E5%AF%86%E5%BA%A6%E8%81%9A%E7%B1%BB"><span class="toc-number">1.5.9.</span> <span class="toc-text">OPTICS密度聚类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Agglomerative%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB"><span class="toc-number">1.5.10.</span> <span class="toc-text">Agglomerative层次聚类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Divisive%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB"><span class="toc-number">1.5.11.</span> <span class="toc-text">Divisive层次聚类</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%BE%E7%AE%97%E6%B3%95"><span class="toc-number">1.6.</span> <span class="toc-text">图算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%91%E7%B9%81%E5%AD%90%E5%9B%BE%E6%8C%96%E6%8E%98%E7%AE%97%E6%B3%95"><span class="toc-number">1.6.1.</span> <span class="toc-text">频繁子图挖掘算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HITS%E7%AE%97%E6%B3%95"><span class="toc-number">1.6.2.</span> <span class="toc-text">HITS算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PageRank%E7%AE%97%E6%B3%95"><span class="toc-number">1.6.3.</span> <span class="toc-text">PageRank算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AF%BB%E6%89%BE%E6%9C%80%E4%BC%98%E8%A7%A3"><span class="toc-number">1.7.</span> <span class="toc-text">寻找最优解</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95%EF%BC%88GA%EF%BC%89"><span class="toc-number">1.7.1.</span> <span class="toc-text">遗传算法（GA）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%9A%81%E7%BE%A4%E7%AE%97%E6%B3%95%EF%BC%88ACO%EF%BC%89"><span class="toc-number">1.7.2.</span> <span class="toc-text">蚁群算法（ACO）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95%EF%BC%88viterbi%EF%BC%89"><span class="toc-number">1.7.3.</span> <span class="toc-text">维特比算法（viterbi）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E6%8B%9F%E9%80%80%E7%81%AB%E7%AE%97%E6%B3%95"><span class="toc-number">1.7.4.</span> <span class="toc-text">模拟退火算法</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87"><span class="toc-number">2.</span> <span class="toc-text">评价指标</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%84%E4%BB%B7%E5%9B%9E%E5%BD%92"><span class="toc-number">2.1.</span> <span class="toc-text">评价回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#SSE%EF%BC%88%E6%AE%8B%E5%B7%AE%E5%B9%B3%E6%96%B9%E5%92%8C%E3%80%81%E5%92%8C%E6%96%B9%E5%B7%AE%EF%BC%89"><span class="toc-number">2.1.1.</span> <span class="toc-text">SSE（残差平方和、和方差）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MSE%EF%BC%88%E5%9D%87%E6%96%B9%E5%B7%AE%E3%80%81%E6%96%B9%E5%B7%AE%EF%BC%89"><span class="toc-number">2.1.2.</span> <span class="toc-text">MSE（均方差、方差）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RMSE%EF%BC%88%E5%9D%87%E6%96%B9%E6%A0%B9%E3%80%81%E6%A0%87%E5%87%86%E5%B7%AE%EF%BC%89"><span class="toc-number">2.1.3.</span> <span class="toc-text">RMSE（均方根、标准差）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MAE%EF%BC%88%E5%B9%B3%E5%9D%87%E7%BB%9D%E5%AF%B9%E8%AF%AF%E5%B7%AE%EF%BC%89"><span class="toc-number">2.1.4.</span> <span class="toc-text">MAE（平均绝对误差）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MPAE%EF%BC%88%E5%B9%B3%E5%9D%87%E7%BB%9D%E5%AF%B9%E7%99%BE%E5%88%86%E6%AF%94%E8%AF%AF%E5%B7%AE%EF%BC%89"><span class="toc-number">2.1.5.</span> <span class="toc-text">MPAE（平均绝对百分比误差）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sMAPE-%E5%AF%B9%E7%A7%B0%E5%B9%B3%E5%9D%87%E7%BB%9D%E5%AF%B9%E7%99%BE%E5%88%86%E6%AF%94%E8%AF%AF%E5%B7%AE"><span class="toc-number">2.1.6.</span> <span class="toc-text">sMAPE 对称平均绝对百分比误差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#R-squared%EF%BC%88%E7%A1%AE%E5%AE%9A%E7%B3%BB%E6%95%B0%EF%BC%89"><span class="toc-number">2.1.7.</span> <span class="toc-text">R-squared（确定系数）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Adjusted-R-squared%EF%BC%88%E8%B0%83%E6%95%B4R%E6%96%B9%EF%BC%89"><span class="toc-number">2.1.8.</span> <span class="toc-text">Adjusted R-squared（调整R方）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Huber-Loss%EF%BC%88%E9%9C%8D%E5%B0%94%E4%BC%AF%E6%8D%9F%E5%A4%B1%EF%BC%89"><span class="toc-number">2.1.9.</span> <span class="toc-text">Huber Loss（霍尔伯损失）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%84%E4%BB%B7%E8%81%9A%E7%B1%BB"><span class="toc-number">2.2.</span> <span class="toc-text">评价聚类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BD%AE%E5%BB%93%E7%B3%BB%E6%95%B0"><span class="toc-number">2.2.1.</span> <span class="toc-text">轮廓系数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Calinski-Harabasz%E6%8C%87%E6%95%B0%EF%BC%88%E6%96%B9%E5%B7%AE%E6%AF%94%E5%87%86%E5%88%99%EF%BC%89"><span class="toc-number">2.2.2.</span> <span class="toc-text">Calinski-Harabasz指数（方差比准则）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Davies-Bouldin-Index%EF%BC%88DB%E6%8C%87%E6%95%B0%EF%BC%89"><span class="toc-number">2.2.3.</span> <span class="toc-text">Davies-Bouldin Index（DB指数）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Rand-Index%EF%BC%88%E5%85%B0%E5%BE%B7%E6%8C%87%E6%95%B0%EF%BC%89"><span class="toc-number">2.2.4.</span> <span class="toc-text">Rand Index（兰德指数）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Adjusted-Rand-Score%EF%BC%88%E8%B0%83%E6%95%B4%E5%85%B0%E5%BE%B7%E6%8C%87%E6%95%B0%EF%BC%89"><span class="toc-number">2.2.5.</span> <span class="toc-text">Adjusted Rand Score（调整兰德指数）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Normalized-Mutual-Information-Score%EF%BC%88%E6%A0%87%E5%87%86%E5%8C%96%E4%BA%92%E4%BF%A1%E6%81%AF%E5%88%86%E6%95%B0%EF%BC%89"><span class="toc-number">2.2.6.</span> <span class="toc-text">Normalized Mutual Information Score（标准化互信息分数）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Adjusted-Mutual-Information-Score%EF%BC%88%E8%B0%83%E6%95%B4%E4%BA%92%E4%BF%A1%E6%81%AF%E5%88%86%E6%95%B0%EF%BC%89"><span class="toc-number">2.2.7.</span> <span class="toc-text">Adjusted Mutual Information Score（调整互信息分数）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Homogeneity-and-Completeness-Score%EF%BC%88%E5%90%8C%E8%B4%A8%E6%80%A7%E5%92%8C%E5%AE%8C%E6%95%B4%E6%80%A7%EF%BC%89"><span class="toc-number">2.2.8.</span> <span class="toc-text">Homogeneity and Completeness Score（同质性和完整性）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#V-Measure"><span class="toc-number">2.2.9.</span> <span class="toc-text">V-Measure</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Fowlkes-Mallows-Score"><span class="toc-number">2.2.10.</span> <span class="toc-text">Fowlkes-Mallows Score</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%82%93%E6%81%A9%EF%BC%88dunn%EF%BC%89%E6%8C%87%E6%A0%87"><span class="toc-number">2.2.11.</span> <span class="toc-text">邓恩（dunn）指标</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%84%E4%BB%B7%E5%88%86%E7%B1%BB"><span class="toc-number">2.3.</span> <span class="toc-text">评价分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5"><span class="toc-number">2.3.1.</span> <span class="toc-text">混淆矩阵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%87%86%E7%A1%AE%E7%8E%87%EF%BC%88Accuracy%EF%BC%89"><span class="toc-number">2.3.2.</span> <span class="toc-text">准确率（Accuracy）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%B2%BE%E7%A1%AE%E7%8E%87%EF%BC%88Precision%EF%BC%89"><span class="toc-number">2.3.3.</span> <span class="toc-text">精确率（Precision）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%AC%E5%9B%9E%E7%8E%87%EF%BC%88Recall%EF%BC%89"><span class="toc-number">2.3.4.</span> <span class="toc-text">召回率（Recall）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#F1-Score"><span class="toc-number">2.3.5.</span> <span class="toc-text">F1-Score</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ROC%E6%9B%B2%E7%BA%BF"><span class="toc-number">2.3.6.</span> <span class="toc-text">ROC曲线</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AUC%E5%80%BC"><span class="toc-number">2.3.7.</span> <span class="toc-text">AUC值</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hinge-Loss%EF%BC%88%E9%93%B0%E9%93%BE%E6%8D%9F%E5%A4%B1%EF%BC%89"><span class="toc-number">2.3.8.</span> <span class="toc-text">Hinge Loss（铰链损失）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Log-Loss%EF%BC%88%E5%AF%B9%E6%95%B0%E6%8D%9F%E5%A4%B1%EF%BC%89"><span class="toc-number">2.3.9.</span> <span class="toc-text">Log Loss（对数损失）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Softmax%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1"><span class="toc-number">2.3.10.</span> <span class="toc-text">Softmax交叉熵损失</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kappa%E7%B3%BB%E6%95%B0"><span class="toc-number">2.3.11.</span> <span class="toc-text">kappa系数</span></a></li></ol></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background: transparent"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By TnTeQAQ</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>